---
layout: single
class: wide
title:  "Attention"
typora-root-url: ../
categories: study
published: true
tag: [NLP, python, study]
toc: true
author_profile: false
sidebar: 
nav: "docs"
---


# 1. Attention Mechanism 개요
Attention은 기계 번역을 할 때 문장에서 번역하고자 하는 언어의 모든 단어를 주목하는 것이 아니라, 어떤 특정 단어들을 중요하게 취급(주목, Attention)해야 하는지를 결정하기 위하여 개발되었다.
## 1.1. Attention Map
기존에는 순서가 존재하는 RNN(순차적으로 학습)의 Encoder와 Decoder를 이용하여 진행하였기 때문에 번역하고자 하는 언어의 단어와 번역된 언어의 단어간의 직접적인 관계를 파악하기 어려웠다. Attention은 단어와 단어간의 직접적인 관계를 파악할 수 있으며, Attention Map 은 이것을 시각화 한 그림으로 단어의 흐름을 파악할 수 있다(흰색 -> Attention이 더 많이 들어가 있는 것). 

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/fa57d048-fcef-41d2-b7dd-3c4aff3b82ff){: width="70%" height="70%"}


출처: Bahdanau et al. 2015 “Neural machine translation by jointly learning to align and translate”

## 1.2. Mechanism
기존 모델에서는 Encoder에서 추출된 하나의 vector(전체 문장의 정보가 함축되어 있음)가 Decoder의 hidden state로 작동하였다.
수식적으로는 argmax(p(y|x))로 I am a student라는 문장이 입력되었을 때 Je suis etudiant라는 문장이 나올 확률을 가장 높일 수 있도록 y에 대한 parameter를 최적화하며 학습을 진행한다. end vector 하나로 Decoder를 진행함으로써 발생하는 문제를 long-range dependency problem라고 하는데, 현재의 time step과 멀어지면 멀어질수록 정보의 손실이 커짐을 의미한다. 

이러한 문제점을 해결하기 위하여 Attention 방법에서는 context vector를 만들어 전체 정보를 활용할 수 있도록 보완하였다. 아래 그림에서 볼 수 있듯 I am a student 문장에서의 모든 단어를 할용할 수 있도록 hidden state를 구성한다. 예를 들면 Je라는 단어에서 suis라는 단어를 예측할 때와 그리고 suis라는 단어에서 etudiant라는 단어를 예측할 때마다 context vector를 새롭게 계산하여 사용한다. 

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/7c17817f-3891-4422-9e00-24bbcd1ed6d8)

출처: https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg?hl=ko

Context vector를 계산하는 방법은 다음과 같다.
먼저 Ju라는 단어에 대하여 I, am, a, student 단어들(key)과 attention weight(유사도 개념으로 보통 Bahdanau Attention을 사용)를 계산한다. 계산된 weight를 이용하여 I, am, a, student 단어들(value)과 다시 가중합을 구하고 이를 convext vector로 사용하여 다음에 올 단어가 무엇인지 예측한다. 그리고 이 과정을 반복하여 수행한다.

Attention 방법은 long-range dependency problem을 해결하여 문장이 길더라도 좋은 성능을 보여주었으며, 해석가능한 모델이라는 점에 그 장점이 있다.

## 1.3. Luong vs Bahdanau

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/c8d94a2e-bdae-4a7e-992e-34d7a59b1689)

출처: https://stackoverflow.com/questions/44238154/what-is-the-difference-between-luong-attention-and-bahdanau-attention

# 2. Transformer
[ATTENTION IS ALL YOU NEED](https://arxiv.org/abs/1706.03762)

Attention 모델을 발전시켜 만들어진 것이 Transformaer 모델인데, 이는 자연어를 이해하는 부분인 Encoder와 자연어를 생성하는 부분인 Decoder로 구성되어 있으며 RNN구조를 사용하는 것이 아닌 행렬 연산으로 계산을 진행한다. 따라서 Trnasformer 모델은 거리에 영향을 받지 않는다. 이러한 구조에서 Encoder를 이용한 모델이 Bert이고 Decoder를 이용한 모델이 GPT이다. 

Attention 개념만 가저온 모델로서 병렬 연산이 가능해짐(문장 전체를 한 번에 학습)에 따라 학습 속도가 매우 빠르고, 단어간의 관계를 Attention 모델보다 정확하게 파악할 수 있다는 장점이 존재한다. 

## 2.1. Attention map
input과 output이 동일한 self attention을 수행해며, 한 단어에 대하여 여러 특징을 추출하기 위하여 다양하게 map을 생성한다.

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/e243b564-acf7-4151-94d7-8031e5aa4578)

출처: https://data-science-blog.com/wp-content/uploads/2021/12/Transformer_head_img-1030x161.png


## 2.2. Transformer의 기본 구조 
Attention과 동일하게 seq2seq(Encoder - Decoder)구조를 가지고 있지만, Attention Map을 생성할 때 재귀적으로 학습이 이뤄지는 것이 아니라 한번에 연산을 수행한다.

아래 모델은 2개의 sub-layer를 가진 Encoder와 3개의 sub-layer를 가진 Decoder로 이루어져 있으며, 해당 연산을 수행할 때 input과 output의 shape(tensor;batch size, seq of length, embedding size)는 모두 동일하다.

실질적으로 Attention을 수행하는 것은 오른편의 Multi-Head Attention과 Scaled Dot-product Attention에서 진행되며, Scaled Dot-product Attention의 결과에서 Attention Map을 얻을 수 있다. 또한, Transformer를 이해하기 위해서는 Attention Mask, Positive-wise FFNN, Encoder구조, Residual Connection & Layer Normalization, Decoder의 구조, Encoder-decoder Self Attention의 개념을 이해하여야 한다.

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/17a914fa-027e-43c1-8584-90dfd887c8e3)

https://deepfrench.gitlab.io/deep-learning-project/resources/transformer.png

## 2.3. Input 데이터
데이터에 대하여 self-attention을 진행하기에 앞서, 순서나 위치 정보를 파악하기 위하여 아래와 같이 position encoding을 진행한다. 

![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/ce80b632-8da9-4ae5-8aca-34fc5c8a55b3)

출처: https://www.scaler.com/topics/images/positionalencodingexampletable.webp

고유한 위치 정보를 보존하기 위해 d차원의 벡터를 생성하는데, 이때 i 값을 그대로 사용하게 되면 문장이 길어질 수록 값이 너무 커지기 때문에, i 주기함수(sin, cos)를 사용하여 개선하였다. 각 포지션 마다 다른 주기를 부여(i가 커질수록 주기가 길어지도록)하여 동일한 위치 embedding이 입력되더라도 다른 값을 출력할 수 있도록 하였다.

https://www.scaler.com/topics/images/sin-curve1.webp![image](https://github.com/yujinp0/yujinp0.github.io/assets/138744622/646e9d32-0fd3-4a04-b720-50538f71abbc)

Attention Map 은 단어가 지니고 있는 여러 특징들을 파악할 수 있도록 다수의 attention map을 만들어 내는데 이를 위하여 Input 데이터를 head 수에 맞춰 아래와 같이 변형해 주어야 한다.
(batch size, seq of length, embedding size) -> (batch size, head size, seq of length, embedding size/head size) 

한번에 진행되는 것은 아니고 resahpe과정과 transform 과정이 수반된다.
1) original: (batch size, seq of length, embedding size)
2) reshape: (batch size, seq of length, head size, embedding size/head size)
3) transform: (batch size, head size, seq of length, embedding size/head size) 




## 2.4. Transformer의 구조 상세 
### 2.2.1. Multi-Head Attention
Head를 여러개(8개 가정)로 쪼개어, 여러 Attention Map 을 만들 수 있도록 함.
병렬연산을 가능하게 함

### 2.2.2. Scaled Dot-product Attention


### 2.2.3. Attention Mask


### 2.2.4. Positive-wise FFNN


### 2.2.5. Encoder구조
self-attention

### 2.2.6. Residual Connection & Layer Normalization


### 2.2.7. Decoder의 구조
self-attention + Masking(정답 가리기)

### 2.2.8. Encoder-decoder Self Attention





# 3. Code

## 3.1. Attention code


```python
import tensorflow as tf

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from sklearn.model_selection import train_test_split

import unicodedata
import re
import numpy as np
import os
import io
import time

# 파일을 다운로드합니다.
path_to_zip = tf.keras.utils.get_file(
    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',
    extract=True)

path_to_file = os.path.dirname(path_to_zip)+"/spa-eng/spa.txt"

# 유니코드 파일을 아스키 코드 파일로 변환합니다.
def unicode_to_ascii(s):
  return ''.join(c for c in unicodedata.normalize('NFD', s)
      if unicodedata.category(c) != 'Mn')


def preprocess_sentence(w):
  w = unicode_to_ascii(w.lower().strip())

  # 단어와 단어 뒤에 오는 구두점(.)사이에 공백을 생성합니다.
  # 예시: "he is a boy." => "he is a boy ."
  # 참고:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation
  w = re.sub(r"([?.!,¿])", r" \1 ", w)
  w = re.sub(r'[" "]+', " ", w)

  # (a-z, A-Z, ".", "?", "!", ",")을 제외한 모든 것을 공백으로 대체합니다.
  w = re.sub(r"[^a-zA-Z?.!,¿]+", " ", w)

  w = w.strip()

  # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해서
  # 문장에 start와 end 토큰을 추가합니다.
  w = '<start> ' + w + ' <end>'
  return w

en_sentence = u"May I borrow this book?"
sp_sentence = u"¿Puedo tomar prestado este libro?"
print(preprocess_sentence(en_sentence))
print(preprocess_sentence(sp_sentence).encode('utf-8'))

# 1. 문장에 있는 억양을 제거합니다.
# 2. 불필요한 문자를 제거하여 문장을 정리합니다.
# 3. 다음과 같은 형식으로 문장의 쌍을 반환합니다: [영어, 스페인어]
def create_dataset(path, num_examples):
  lines = io.open(path, encoding='UTF-8').read().strip().split('\n')

  word_pairs = [[preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]

  return zip(*word_pairs)

en, sp = create_dataset(path_to_file, None)
print(en[-1])
print(sp[-1])

def tokenize(lang):
  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(
      filters='')
  lang_tokenizer.fit_on_texts(lang)

  tensor = lang_tokenizer.texts_to_sequences(lang)

  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,
                                                         padding='post')

  return tensor, lang_tokenizer

# 언어 데이터셋을 아래의 크기로 제한하여 훈련과 검증을 수행합니다.
num_examples = 30000
input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)

# 타겟 텐서와 입력 텐서의 최대 길이를 계산합니다.
max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]

# 훈련 집합과 검증 집합을 80대 20으로 분리합니다.
input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)

# 훈련 집합과 검증 집합의 데이터 크기를 출력합니다.
print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))

def convert(lang, tensor):
  for t in tensor:
    if t!=0:
      print ("%d ----> %s" % (t, lang.index_word[t]))

print ("Input Language; index to word mapping")
convert(inp_lang, input_tensor_train[0])
print ()
print ("Target Language; index to word mapping")
convert(targ_lang, target_tensor_train[0])

BUFFER_SIZE = len(input_tensor_train)
BATCH_SIZE = 64
steps_per_epoch = len(input_tensor_train)//BATCH_SIZE
embedding_dim = 256
units = 1024
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1

dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)

example_input_batch, example_target_batch = next(iter(dataset))
example_input_batch.shape, example_target_batch.shape

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')

  def call(self, x, hidden):
    x = self.embedding(x)
    output, state = self.gru(x, initial_state = hidden)
    return output, state

  def initialize_hidden_state(self):
    return tf.zeros((self.batch_sz, self.enc_units))

encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)

# 샘플 입력
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)
print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))
print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))

class BahdanauAttention(tf.keras.layers.Layer):
  def __init__(self, units):
    super(BahdanauAttention, self).__init__()
    self.W1 = tf.keras.layers.Dense(units)
    self.W2 = tf.keras.layers.Dense(units)
    self.V = tf.keras.layers.Dense(1)

  def call(self, query, values):
    # 쿼리 은닉 상태(query hidden state)는 (batch_size, hidden size)쌍으로 이루어져 있습니다.
    # query_with_time_axis은 (batch_size, 1, hidden size)쌍으로 이루어져 있습니다.
    # values는 (batch_size, max_len, hidden size)쌍으로 이루어져 있습니다.
    # 스코어(score)계산을 위해 덧셈을 수행하고자 시간 축을 확장하여 아래의 과정을 수행합니다.
    query_with_time_axis = tf.expand_dims(query, 1)

    # score는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다.
    # score를 self.V에 적용하기 때문에 마지막 축에 1을 얻습니다.
    # self.V에 적용하기 전에 텐서는 (batch_size, max_length, units)쌍으로 이루어져 있습니다.
    score = self.V(tf.nn.tanh(
        self.W1(query_with_time_axis) + self.W2(values)))

    # attention_weights는 (batch_size, max_length, 1)쌍으로 이루어져 있습니다. 
    attention_weights = tf.nn.softmax(score, axis=1)

    # 덧셈이후 컨텍스트 벡터(context_vector)는 (batch_size, hidden_size)쌍으로 이루어져 있습니다.
    context_vector = attention_weights * values
    context_vector = tf.reduce_sum(context_vector, axis=1)

    return context_vector, attention_weights

attention_layer = BahdanauAttention(10)
attention_result, attention_weights = attention_layer(sample_hidden, sample_output)

print("Attention result shape: (batch size, units) {}".format(attention_result.shape))
print("Attention weights shape: (batch_size, sequence_length, 1) {}".format(attention_weights.shape))

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.dec_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc = tf.keras.layers.Dense(vocab_size)

    # 어텐션을 사용합니다.
    self.attention = BahdanauAttention(self.dec_units)

  def call(self, x, hidden, enc_output):
    # enc_output는 (batch_size, max_length, hidden_size)쌍으로 이루어져 있습니다.
    context_vector, attention_weights = self.attention(hidden, enc_output)

    # 임베딩층을 통과한 후 x는 (batch_size, 1, embedding_dim)쌍으로 이루어져 있습니다.
    x = self.embedding(x)

    # 컨텍스트 벡터와 임베딩 결과를 결합한 이후 x의 형태는 (batch_size, 1, embedding_dim + hidden_size)쌍으로 이루어져 있습니다.
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # 위에서 결합된 벡터를 GRU에 전달합니다.
    output, state = self.gru(x)

    # output은 (batch_size * 1, hidden_size)쌍으로 이루어져 있습니다.
    output = tf.reshape(output, (-1, output.shape[2]))

    # output은 (batch_size, vocab)쌍으로 이루어져 있습니다.
    x = self.fc(output)

    return x, state, attention_weights

decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)

sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),
                                      sample_hidden, sample_output)

print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))


optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')

def loss_function(real, pred):
  mask = tf.math.logical_not(tf.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tf.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tf.reduce_mean(loss_)

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

@tf.function
def train_step(inp, targ, enc_hidden):
  loss = 0

  with tf.GradientTape() as tape:
    enc_output, enc_hidden = encoder(inp, enc_hidden)

    dec_hidden = enc_hidden

    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)

    # 교사 강요(teacher forcing) - 다음 입력으로 타겟을 피딩(feeding)합니다.
    for t in range(1, targ.shape[1]):
      # enc_output를 디코더에 전달합니다.
      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)

      loss += loss_function(targ[:, t], predictions)

      # 교사 강요(teacher forcing)를 사용합니다.
      dec_input = tf.expand_dims(targ[:, t], 1)

  batch_loss = (loss / int(targ.shape[1]))

  variables = encoder.trainable_variables + decoder.trainable_variables

  gradients = tape.gradient(loss, variables)

  optimizer.apply_gradients(zip(gradients, variables))

  return batch_loss

EPOCHS = 10

for epoch in range(EPOCHS):
  start = time.time()

  enc_hidden = encoder.initialize_hidden_state()
  total_loss = 0

  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
    batch_loss = train_step(inp, targ, enc_hidden)
    total_loss += batch_loss

    if batch % 100 == 0:
      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                   batch,
                                                   batch_loss.numpy()))
  # 에포크가 2번 실행될때마다 모델 저장 (체크포인트)
  if (epoch + 1) % 2 == 0:
    checkpoint.save(file_prefix = checkpoint_prefix)

  print('Epoch {} Loss {:.4f}'.format(epoch + 1,
                                      total_loss / steps_per_epoch))
  print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))

def evaluate(sentence):
  attention_plot = np.zeros((max_length_targ, max_length_inp))

  sentence = preprocess_sentence(sentence)

  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]
  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],
                                                         maxlen=max_length_inp,
                                                         padding='post')
  inputs = tf.convert_to_tensor(inputs)

  result = ''

  hidden = [tf.zeros((1, units))]
  enc_out, enc_hidden = encoder(inputs, hidden)

  dec_hidden = enc_hidden
  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)

  for t in range(max_length_targ):
    predictions, dec_hidden, attention_weights = decoder(dec_input,
                                                         dec_hidden,
                                                         enc_out)

    # 나중에 어텐션 가중치를 시각화하기 위해 어텐션 가중치를 저장합니다.
    attention_weights = tf.reshape(attention_weights, (-1, ))
    attention_plot[t] = attention_weights.numpy()

    predicted_id = tf.argmax(predictions[0]).numpy()

    result += targ_lang.index_word[predicted_id] + ' '

    if targ_lang.index_word[predicted_id] == '<end>':
      return result, sentence, attention_plot

    # 예측된 ID를 모델에 다시 피드합니다.
    dec_input = tf.expand_dims([predicted_id], 0)

  return result, sentence, attention_plot

# 어텐션 가중치를 그리기 위한 함수입니다.
def plot_attention(attention, sentence, predicted_sentence):
  fig = plt.figure(figsize=(10,10))
  ax = fig.add_subplot(1, 1, 1)
  ax.matshow(attention, cmap='viridis')

  fontdict = {'fontsize': 14}

  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

  plt.show()

def translate(sentence):
  result, sentence, attention_plot = evaluate(sentence)

  print('Input: %s' % (sentence))
  print('Predicted translation: {}'.format(result))

  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]
  plot_attention(attention_plot, sentence.split(' '), result.split(' '))

# checkpoint_dir내에 있는 최근 체크포인트(checkpoint)를 복원합니다.
checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))

translate(u'hace mucho frio aqui.')
translate(u'esta es mi vida.')
translate(u'¿todavia estan en casa?')
# 잘못된 번역
translate(u'trata de averiguarlo.')

#-------------------------------
#출처: https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=ko
#-------------------------------

```

